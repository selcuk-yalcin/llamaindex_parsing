"""
Structured Legal Document Extractor
LlamaParse + OpenRouter LLM ile hukuki belgeleri yapÄ±landÄ±rÄ±lmÄ±ÅŸ JSON'a Ã§evirme
"""

import os
import json
import asyncio
from pathlib import Path
from typing import List, Optional
from dotenv import load_dotenv

from llama_parse import LlamaParse
from pydantic_models import LegalDocument
from config import Config

import httpx

load_dotenv()


class StructuredExtractor:
    """LlamaParse + OpenRouter ile yapÄ±landÄ±rÄ±lmÄ±ÅŸ Ã§Ä±ktÄ± Ã¼retici"""
    
    def __init__(self):
        self.parser = LlamaParse(
            api_key=Config.LLAMAPARSE_API_KEY,
            result_type=Config.LLAMAPARSE_RESULT_TYPE,
            language=Config.LLAMAPARSE_LANGUAGE,
            premium_mode=Config.LLAMAPARSE_PREMIUM_MODE,
            verbose=True
        )
        
        self.openrouter_key = Config.OPENROUTER_API_KEY
        self.model = Config.DEFAULT_MODEL
        
    async def parse_pdf(self, pdf_path: str) -> str:
        """PDF'i markdown'a Ã§evir"""
        print(f"ğŸ“„ Parsing: {pdf_path}")
        documents = await self.parser.aload_data(pdf_path)
        
        # TÃ¼m sayfa iÃ§eriklerini birleÅŸtir
        full_text = "\n\n".join([doc.text for doc in documents])
        print(f"âœ… Parsed {len(documents)} pages")
        return full_text
    
    async def extract_structured_json(self, markdown_text: str, filename: str) -> dict:
        """Markdown metni yapÄ±landÄ±rÄ±lmÄ±ÅŸ JSON'a Ã§evir"""
        print("ğŸ¤– LLM ile yapÄ±landÄ±rma baÅŸlatÄ±lÄ±yor...")
        
        # Metin uzunluÄŸunu kontrol et
        text_length = len(markdown_text)
        print(f"ğŸ“ Metin uzunluÄŸu: {text_length:,} karakter")
        
        # Ã‡ok uzunsa kÄ±salt (context window limiti iÃ§in)
        max_text_length = 200000  # ~200k karakter
        if text_length > max_text_length:
            print(f"âš ï¸ Metin Ã§ok uzun, ilk {max_text_length:,} karakter kullanÄ±lÄ±yor")
            markdown_text = markdown_text[:max_text_length]
        
        # JSON ÅŸemasÄ±nÄ± al
        schema = LegalDocument.model_json_schema()
        
        # LLM prompt'u
        system_prompt = """Sen bir hukuki belge analiz uzmanÄ±sÄ±n. 
Verilen TÃ¼rkÃ§e kanun/yÃ¶netmelik metnini JSON ÅŸemasÄ±na gÃ¶re yapÄ±landÄ±r.

KURALLAR:
1. TÃ¼m maddeleri hiyerarÅŸik olarak Ã§Ä±kar
2. Kanun metadata'sÄ±nÄ± ekle (baÅŸlÄ±k, numara, tarih)
3. TanÄ±mlarÄ± ayrÄ± array'e al
4. Ceza hÃ¼kÃ¼mlerini penalties'e ekle
5. Madde atÄ±flarÄ±nÄ± tespit et
6. SADECE VALID JSON dÃ¶ndÃ¼r"""

        user_prompt = f"""Belge: {filename}

METIN:
{markdown_text}

ÅEMA:
{json.dumps(schema, indent=2, ensure_ascii=False)[:5000]}

YukarÄ±daki metni JSON olarak yapÄ±landÄ±r."""

        # OpenRouter API Ã§aÄŸrÄ±sÄ±
        async with httpx.AsyncClient(timeout=300.0) as client:
            response = await client.post(
                f"{Config.OPENROUTER_BASE_URL}/chat/completions",
                headers={
                    "Authorization": f"Bearer {self.openrouter_key}",
                    "HTTP-Referer": "https://github.com/your-repo",
                    "X-Title": "Legal Document Extractor"
                },
                json={
                    "model": self.model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 100000,  # Increased for long documents
                    "response_format": {"type": "json_object"}
                }
            )
            
            if response.status_code != 200:
                raise Exception(f"OpenRouter API error: {response.status_code} - {response.text}")
            
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            # Debug: JSON uzunluÄŸunu yazdÄ±r
            print(f"ğŸ“Š JSON yanÄ±t uzunluÄŸu: {len(content)} karakter")
            
            # JSON parse et - hata yÃ¶netimi ile
            try:
                structured_data = json.loads(content)
                print("âœ… YapÄ±landÄ±rma tamamlandÄ±")
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSON parse hatasÄ±: {e}")
                print(f"ğŸ” YanÄ±tÄ±n ilk 500 karakteri:\n{content[:500]}")
                print(f"ğŸ” YanÄ±tÄ±n son 500 karakteri:\n{content[-500:]}")
                
                # Fallback: Eksik JSON'u tamamlamaya Ã§alÄ±ÅŸ
                print("ğŸ”§ JSON dÃ¼zeltme deneniyor...")
                # En basit Ã§Ã¶zÃ¼m: JSON'u temizle ve tekrar parse et
                import re
                # Trailing comma'larÄ± temizle
                content = re.sub(r',(\s*[}\]])', r'\1', content)
                try:
                    structured_data = json.loads(content)
                    print("âœ… DÃ¼zeltme baÅŸarÄ±lÄ±!")
                except:
                    print("âŒ JSON dÃ¼zeltilemedi, ham veri kaydediliyor")
                    raise
            
            return structured_data
    
    async def process_file(self, pdf_path: str, output_dir: str = "extracted_laws") -> str:
        """Tek bir PDF dosyasÄ±nÄ± iÅŸle"""
        pdf_file = Path(pdf_path)
        
        # 1. PDF'i parse et
        markdown_text = await self.parse_pdf(str(pdf_file))
        
        # 2. LLM ile yapÄ±landÄ±r
        try:
            structured_data = await self.extract_structured_json(
                markdown_text, 
                pdf_file.stem
            )
        except Exception as e:
            print(f"âŒ LLM yapÄ±landÄ±rma hatasÄ±: {e}")
            print("ğŸ’¡ Raw markdown kaydediliyor...")
            
            # Raw markdown'Ä± kaydet
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)
            markdown_file = output_path / f"{pdf_file.stem}_raw.md"
            with open(markdown_file, 'w', encoding='utf-8') as f:
                f.write(markdown_text)
            print(f"ğŸ’¾ Raw markdown: {markdown_file}")
            raise
        
        # 3. Pydantic ile validate et
        try:
            validated = LegalDocument(**structured_data)
            final_json = validated.model_dump()
            print("âœ… Validation baÅŸarÄ±lÄ±")
        except Exception as e:
            print(f"âš ï¸ Validation hatasÄ±: {e}")
            print("ğŸ”§ Raw JSON kullanÄ±lÄ±yor...")
            final_json = structured_data
        
        # 4. JSON dosyasÄ±nÄ± kaydet
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        output_file = output_path / f"{pdf_file.stem}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_json, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ JSON kaydedildi: {output_file}")
        
        return str(output_file)
    
    async def process_directory(self, input_dir: str, output_dir: str = "extracted_laws"):
        """Bir klasÃ¶rdeki tÃ¼m PDF'leri iÅŸle"""
        input_path = Path(input_dir)
        pdf_files = list(input_path.glob("*.pdf"))
        
        if not pdf_files:
            print(f"âŒ {input_dir} iÃ§inde PDF bulunamadÄ±")
            return
        
        print(f"ğŸ“ {len(pdf_files)} PDF bulundu")
        
        for pdf_file in pdf_files:
            try:
                await self.process_file(str(pdf_file), output_dir)
                print("-" * 80)
            except Exception as e:
                print(f"âŒ Hata ({pdf_file.name}): {e}")
                continue


async def main():
    """Ana fonksiyon - test iÃ§in"""
    extractor = StructuredExtractor()
    
    # Ã–rnek: data klasÃ¶rÃ¼ndeki tÃ¼m PDF'leri iÅŸle
    data_dir = Path(__file__).parent / "data"
    
    if data_dir.exists():
        await extractor.process_directory(str(data_dir))
    else:
        print(f"âŒ {data_dir} klasÃ¶rÃ¼ bulunamadÄ±")
        print("ğŸ’¡ KullanÄ±m: data/ klasÃ¶rÃ¼ne PDF dosyalarÄ±nÄ±zÄ± ekleyin")


if __name__ == "__main__":
    asyncio.run(main())

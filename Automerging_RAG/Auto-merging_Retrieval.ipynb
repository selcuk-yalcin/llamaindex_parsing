{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd7996c5-91eb-41de-ac6a-0b22462b76f1",
   "metadata": {},
   "source": [
    "# Auto-merging Retrieval System\n",
    "\n",
    "# **Overview**\n",
    "This notebook implements **Auto-merging Retrieval**, an advanced retrieval strategy that combines hierarchical document chunking with intelligent merging during query time. It automatically merges smaller chunks into larger ones when appropriate, providing better context while maintaining efficiency.\n",
    "\n",
    "**Key Components**\n",
    "\n",
    "### 1. **Hierarchical Node Parser**\n",
    "- **Chunk Sizes**: [2048, 512, 128] tokens\n",
    "- **Multi-level hierarchy**: Parent ‚Üí Child ‚Üí Leaf nodes\n",
    "- **Smart merging**: Automatically combines related chunks\n",
    "\n",
    "### 2. **Auto-merging Retriever**\n",
    "- **Similarity Search**: Top-12 most relevant chunks\n",
    "- **Intelligent Merging**: Combines small chunks into meaningful contexts\n",
    "- **Context Preservation**: Maintains document structure and relationships\n",
    "\n",
    "### 3. **Re-ranking**\n",
    "- **SentenceTransformerRerank**: Uses BAAI/bge-reranker-base\n",
    "- **Top-6 Selection**: Re-ranks and selects best 6 chunks\n",
    "- **Quality Enhancement**: Improves retrieval precision\n",
    "\n",
    "## **Use Cases**\n",
    "- **Long Documents**: Better context for large PDFs/books\n",
    "- **Structured Content**: Maintains document hierarchy\n",
    "- **Precision Tasks**: When exact context is crucial\n",
    "- **Research Papers**: Academic document analysis and so on ....\n",
    "\n",
    "## **Advantages**\n",
    "- **Better Context**: Merges related information automatically\n",
    "- **Flexible Retrieval**: Adapts chunk size based on query needs\n",
    "- **Memory Efficient**: Avoids storing all chunk combinations\n",
    "- **High Precision**: Re-ranking improves answer quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20373372-03c5-41e9-8bc9-dfa6ce35d666",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(f\"üêç Using Python: {sys.executable}\")\n",
    "print(f\"üìç Python version: {sys.version}\")\n",
    "\n",
    "# Fix NumPy/Pandas binary compatibility issue\n",
    "print(\"üîß Ensuring NumPy/Pandas compatibility...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"-q\", \"pandas\"])\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-cache-dir\", \"pandas==2.0.3\"])\n",
    "    print(\"‚úÖ Pandas reinstalled with NumPy 1.24.3 compatibility\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Pandas already compatible\")\n",
    "\n",
    "# Install basic packages\n",
    "print(\"üì¶ Installing other packages...\")\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"matplotlib\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"docx2txt\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"python-dotenv\"])\n",
    "\n",
    "print(\"‚úÖ Package installation complete!\")\n",
    "\n",
    "# All imports\n",
    "import warnings\n",
    "import os\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# LlamaIndex Core imports\n",
    "from llama_index.core import SimpleDirectoryReader, Document, VectorStoreIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core.settings import Settings\n",
    "\n",
    "# LlamaIndex Node Parser imports\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser, get_leaf_nodes\n",
    "\n",
    "# LlamaIndex LLM imports\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# LlamaIndex Retriever and Postprocessor imports\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# LlamaIndex Response Utils\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys from .env file (already loaded with load_dotenv() above)\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fc195-9619-4088-831e-66b3cc6e0425",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from llama_index.core import SimpleDirectoryReader  # Already imported at the top\n",
    "\n",
    "# Load all files from the data directory\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=\"./data\"\n",
    ").load_data()\n",
    "\n",
    "# If you want to load a single specific PDF file, uncomment below:\n",
    "# documents = SimpleDirectoryReader(\n",
    "#     input_files=[\"./data/YAPI ƒ∞≈ûLERƒ∞NDE ƒ∞≈û SAƒûLIƒûI VE G√úVENLƒ∞ƒûƒ∞ Y√ñNETMELƒ∞ƒûƒ∞.pdf\"]\n",
    "# ).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad555b-48d4-441b-b5f3-79a685b4c3f2",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(documents), \"\\n\")\n",
    "print(len(documents), \"\\n\")\n",
    "print(type(documents[0]))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c143850-72c6-4a09-9fb0-a216108314c0",
   "metadata": {},
   "source": [
    "## Auto-merging retrieval setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926348c-a4c5-471b-99ed-e6bf170f1fe1",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from llama_index.core import Document  # Already imported\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ccfb5-cf94-48a4-b23b-8c5a5ab73ebc",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from llama_index.core.node_parser import HierarchicalNodeParser  # Already imported \n",
    "\n",
    "# create the hierarchical node parser w/ default settings\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672fd574-6b3c-43ad-9a9a-2db423cadd48",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cfc301-1887-4b1c-a8aa-1fb55e30be78",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from llama_index.core.node_parser import get_leaf_nodes  # Already imported\n",
    "\n",
    "leaf_nodes = get_leaf_nodes(nodes)\n",
    "print(leaf_nodes[30].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703abeb-3bcc-4dd6-8010-3bcf95d8b483",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes_by_id = {node.node_id: node for node in nodes}\n",
    "\n",
    "parent_node = nodes_by_id[leaf_nodes[30].parent_node.node_id]\n",
    "print(parent_node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bafef6-02f6-4029-a5eb-dfbc4369da56",
   "metadata": {},
   "source": [
    "### Building the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0350cd-392e-45e9-8664-ff22a39a8bcd",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from llama_index.llms.openai import OpenAI  # Already imported \n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03213595-6581-47f5-90a7-cfa381d42872",
   "metadata": {
    "height": 132,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from llama_index.core import ServiceContext  # Deprecated, using Settings instead\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = \"local:BAAI/bge-small-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f84886-e1aa-40f1-a812-4b6f8d0204af",
   "metadata": {
    "height": 183,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from llama_index.core import VectorStoreIndex, StorageContext  # Already imported at the top\n",
    "\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "automerging_index = VectorStoreIndex(\n",
    "    leaf_nodes, storage_context=storage_context\n",
    ")\n",
    "\n",
    "automerging_index.storage_context.persist(persist_dir=\"./merging_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ddbff-6fdd-448d-9ce8-53ed782a6d66",
   "metadata": {
    "height": 438,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This block of code is optional to check\n",
    "# if an index file exist, then it will load it\n",
    "# if not, it will rebuild it\n",
    "\n",
    "# import os  # Already imported \n",
    "# from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage  # Already imported\n",
    "\n",
    "if not os.path.exists(\"./merging_index\"):\n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "    automerging_index = VectorStoreIndex(\n",
    "            leaf_nodes,\n",
    "            storage_context=storage_context\n",
    "        )\n",
    "\n",
    "    automerging_index.storage_context.persist(persist_dir=\"./merging_index\")\n",
    "else:\n",
    "    automerging_index = load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=\"./merging_index\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2710c689-0596-45c5-a63f-1e8bb9481846",
   "metadata": {},
   "source": [
    "### Defining the retriever and running the query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443d25a-2a5e-425c-9cca-4bf8503c6db3",
   "metadata": {
    "height": 336,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from llama_index.core.postprocessor import SentenceTransformerRerank  # Already imported at the top\n",
    "# from llama_index.core.retrievers import AutoMergingRetriever  # Already imported at the top\n",
    "# from llama_index.core.query_engine import RetrieverQueryEngine  # Already imported at the top\n",
    "\n",
    "automerging_retriever = automerging_index.as_retriever(\n",
    "    similarity_top_k=12\n",
    ")\n",
    "\n",
    "retriever = AutoMergingRetriever(\n",
    "    automerging_retriever, \n",
    "    automerging_index.storage_context, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "rerank = SentenceTransformerRerank(top_n=6, model=\"BAAI/bge-reranker-base\")\n",
    "\n",
    "auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "    automerging_retriever, node_postprocessors=[rerank]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458d0ac-24a0-430b-acc4-c05252134527",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "auto_merging_response = auto_merging_engine.query(\n",
    "    \"Yapi isleri yonetmeligine gore kac metreden sonra yuksekte calisma sayilir?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e70b4-daae-4181-a953-7ea37f4620c7",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# from llama_index.core.response.notebook_utils import display_response  # Already imported   \n",
    "display_response(auto_merging_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841036a-0bb6-4037-920e-ae555748e111",
   "metadata": {},
   "source": [
    "## Putting it all Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949da64-bc7f-4929-b394-51d11732e62f",
   "metadata": {
    "height": 1084
   },
   "outputs": [],
   "source": [
    "# import os  # Already imported at the top\n",
    "# from llama_index.core import StorageContext, VectorStoreIndex, load_index_from_storage  # Already imported \n",
    "# from llama_index.core.node_parser import HierarchicalNodeParser, get_leaf_nodes  # Already imported \n",
    "# from llama_index.core.retrievers import AutoMergingRetriever  # Already imported \n",
    "# from llama_index.core.postprocessor import SentenceTransformerRerank  # Already imported \n",
    "# from llama_index.core.query_engine import RetrieverQueryEngine  # Already imported \n",
    "\n",
    "\n",
    "def build_automerging_index(\n",
    "    documents,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"merging_index\",\n",
    "    chunk_sizes=None,\n",
    "):\n",
    "    chunk_sizes = chunk_sizes or [2048, 512, 128]\n",
    "    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "    \n",
    "    # Use Settings instead of deprecated ServiceContext\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = embed_model\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults()\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "    if not os.path.exists(save_dir):\n",
    "        automerging_index = VectorStoreIndex(\n",
    "            leaf_nodes, storage_context=storage_context\n",
    "        )\n",
    "        automerging_index.storage_context.persist(persist_dir=save_dir)\n",
    "    else:\n",
    "        automerging_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=save_dir)\n",
    "        )\n",
    "    return automerging_index\n",
    "\n",
    "\n",
    "def get_automerging_query_engine(\n",
    "    automerging_index,\n",
    "    similarity_top_k=12,\n",
    "    rerank_top_n=6,\n",
    "):\n",
    "    base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "    retriever = AutoMergingRetriever(\n",
    "        base_retriever, automerging_index.storage_context, verbose=True\n",
    "    )\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "    auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "        retriever, node_postprocessors=[rerank]\n",
    "    )\n",
    "    return auto_merging_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f52577-2e8e-4fee-ade9-2f5ce96a7a8c",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "# from llama_index.llms.openai import OpenAI  # Already imported\n",
    "\n",
    "index = build_automerging_index(\n",
    "    [document],\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1),\n",
    "    save_dir=\"./merging_index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753885f-bfaf-4ac0-8410-b62f619827c4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "query_engine = get_automerging_query_engine(index, similarity_top_k=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
